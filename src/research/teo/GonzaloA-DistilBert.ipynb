{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version 2.2.3\n",
      "SpaCy version 3.8.6\n",
      "Torch version 2.7.0+cpu\n",
      "Transformers version 4.51.3\n",
      "MODUL: transformers.training_args\n",
      "DATEI: c:\\Users\\Teo\\Desktop\\Studium\\_Master\\C201 Mustererkennung\\FakeNewsClassification\\.venv\\lib\\site-packages\\transformers\\training_args.py\n",
      "SIGNATUR: (self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, tp_size: Optional[int] = 0, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Version Checks\n",
    "print(\"Pandas version \" + pd.__version__)\n",
    "print(\"SpaCy version \" + spacy.__version__)\n",
    "print(\"Torch version \" + torch.__version__)\n",
    "print(\"Transformers version \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff494026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results\\runs\\May20_19-03-09_LAPTOP-8OT8BTD3,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test Training Arguments (can be skipped)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5420f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (24353, 4)\n",
      "Validation shape: (8117, 4)\n",
      "Test shape: (8117, 4)\n",
      "\n",
      "Train columns: ['Unnamed: 0', 'title', 'text', 'label']\n",
      "   Unnamed: 0                                              title  \\\n",
      "0           0  Palestinians switch off Christmas lights in Be...   \n",
      "1           1  China says Trump call with Taiwan president wo...   \n",
      "2           2   FAIL! The Trump Organization’s Credit Score W...   \n",
      "3           3  Zimbabwe military chief's China trip was norma...   \n",
      "4           4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
      "\n",
      "                                                text  label  \n",
      "0  RAMALLAH, West Bank  - Palestinians switched o...      1  \n",
      "1  BEIJING  - U.S. President-elect Donald Trump’s...      1  \n",
      "2  While the controversy over Trump s personal ta...      0  \n",
      "3  BEIJING  - A trip to Beijing last week by Zimb...      1  \n",
      "4  There has never been a more UNCOURAGEOUS perso...      0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data Paths\n",
    "train_path = \"../../data/GonzaloA - fake_news/train_without_reuters.csv\"\n",
    "val_path   = \"../../data/GonzaloA - fake_news/evaluation_without_reuters.csv\"\n",
    "test_path  = \"../../data/GonzaloA - fake_news/test_without_reuters.csv\"\n",
    "\n",
    "# Read with semicolon separator\n",
    "df_train = pd.read_csv(train_path, sep=';')\n",
    "df_val   = pd.read_csv(val_path,   sep=';')\n",
    "df_test  = pd.read_csv(test_path,  sep=';')\n",
    "\n",
    "# Quick sanity-check\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Validation shape:\", df_val.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "print()\n",
    "print(\"Train columns:\", df_train.columns.tolist())\n",
    "\n",
    "# first 5 rows of dataset\n",
    "print(df_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10f29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b4cb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ramallah west bank   palestinians switch christmas light jesus   traditional birthplace bethlehem wednesday night protest u.s. president donald trump s decision recognize jerusalem israel s capital christmas tree adorn light outside bethlehem s church nativity christian believe jesus bear ramallah burial site palestinian leader yasser arafat plunge darkness   christmas tree switch order mayor today protest trump s decision   say fady ghattas bethlehem s municipal medium officer   say unclear illumination turn main christmas festivity speech washington trump say decide recognize jerusalem israel s capital u.s. embassy city israeli prime minister benjamin netanyahu say trump s   mark beginning new approach israeli palestinian conflict say   historic landmark arabs muslims middle east condemn u.s. decision call incendiary volatile region european union united nations voice alarm possible repercussion chance revive israeli palestinian peacemaking\n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         beijing   u.s. president elect donald trump taiwan president tsai ing wen petty taiwan change status china china taiwan affairs office say saturday china unswervingly stick position oppose taiwan independence say statement release official xinhua news agency\n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                controversy trump s personal tax return continue business credit rating company nav decide look business   credit publish result website nav actually a+ rating better business bureau oppose trump u. s final rating pull factor affect business credit score discover truly laughable high possible business score 100 trump organization s score 19 nineteen 15 20 s actually drop 18 point put company medium high risk category organization consider highly likely default debt average send payment 17 day late derogatorie delinquent loan turn collection tax lien judgment organization accord nav usually mean business defendant lawsuit require pay damage plaintiff compare clinton foundation score 42 42 isn t great s hell lot well trump organization s.*note clinton foundation 1997 know current form year despite have stellar credit clinton foundation derogatorie make payment time thing laughable consider wonderful businessman trump think s laughable consider trump say know debt well august boast   little debt   relative asset regardless s true fact trump organization s credit score low suggest trump nearly good manage debt claim s nearly good manage business claim credit score hilarious feature image scott olson getty images\n",
      "3    beijing   trip beijing week zimbabwe s military chief   normal military exchange china s foreign ministry say wednesday military southern african nation seize power zimbabwe s military take control target   criminal   president robert mugabe give assurance national television 93 year old leader family   safe sound general constantino chiwenga meet chinese defence minister chang wanquan beijing friday chang express willingness promote relation zimbabwe china s defence ministry say short statement week ministry show picture man wear military uniform shake hand officer country sit opposite hold meeting people s liberation army headquarters beijing ask chiwenga brief china plan seize power chinese foreign ministry spokesman geng shuang say defence ministry release information trip didn t understanding specific reception china   tell visit china time normal military exchange mutually agree china zimbabwe   geng say refer question defence ministry respond request comment   country friendly zimbabwe pay close attention development situation zimbabwe   geng add    maintain peaceful stable development accord fundamental interest zimbabwe regional country common desire international community hope relevant party zimbabwe appropriately handle internal matter   contrast elevated status continent mugabe revile west despot disastrous handling economy willingness resort violence maintain power destroy africa s promising state china zimbabwe close diplomatic economic relationship beijing stand mugabe s government face western economic sanction august zimbabwe s government say chinese company plan invest $ 2 billion revive operation zimbabwe iron steel company zisco cease production 2008 height zimbabwe s economic meltdown year china veto propose western back u.n. resolution impose arm embargo zimbabwe financial travel restriction mugabe 13 official say   complicate ease conflict\n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          uncourageous person white house barack obama face decision foreign affair back yes s give iran opportunity nuclear capability remember red line syria?when horrible policy didn t work take blame point idiotic decision forget stimulus boondoggle solendra obamacare take responsibility failure like courageous man receive profile courage award beg congress obamacare trash current president .courage meh!we couldn t voice outrage well katie pavlich frickin   guy president barack obama return spotlight sunday accept award political courage john f. kennedy family day house republicans win passage bill dismantle signature health care law democrat public appearance leave office january avoid mention republican successor white house president donald trump criticize previous administration numerous time move undo obama s initiative obama lady michelle obama arrive john f. kennedy presidential library museum dinner ceremony accept annual profile courage award name 1957 pulitzer prize win book kennedy profile u.s. senator risk career take principle unpopular position long line guest way red carpet library event sunday night member kennedy family member congress obama staffer celebrity include late night talk host david letterman david letterman s that?read nyp\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Test Preprocessing (can be skipped)\n",
    "df_train_sample = df_train[:5]['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Preprocessing\n",
    "df_train['text'] = df_train['text'].astype(str).apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "print(df_train['text'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435dd4eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenizing\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_encodings \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28mlist\u001b[39m(\u001b[43mdf_train\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m test_encodings \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28mlist\u001b[39m(df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_encodings\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizing\n",
    "train_encodings = tokenizer(list(df_train['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(df_test['text']), truncation=True, padding=True)\n",
    "\n",
    "print(train_encodings.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7008cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Translate to Huggingface Dataset Format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'label': list(df_train['label'])\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'label': list(df_test['label'])\n",
    "})\n",
    "\n",
    "# Load Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Define Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), dim=1)\n",
    "print(classification_report(df_test['label'], pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
