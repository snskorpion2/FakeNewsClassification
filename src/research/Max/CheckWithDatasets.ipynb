{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8bc050-bcd2-4a7a-b58c-ad42b71bdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62a8f44-459b-48d5-8268-a9dbf4014379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_datasets(basepath):\n",
    "    \"\"\"\n",
    "    Load and clean individual fake news datasets from subfolders in basepath.\n",
    "    Returns a dict of pandas DataFrames keyed by dataset name.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "\n",
    "    # 1. Aadya Singh_Fakenews: evaluation.csv, test (1).csv, train (2).csv\n",
    "    folder = os.path.join(basepath, \"Aadya Singh  _fake-and_real_news\")\n",
    "    files = [\"evaluation.csv\", \"test (1).csv\", \"train (2).csv\"]\n",
    "    dfs = [pd.read_csv(os.path.join(folder, f), sep=';') for f in files]\n",
    "    datasets['Aadya_Singh'] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 2. Bhavik Jikadara - Fake News Detection\n",
    "    folder = os.path.join(basepath, \"Bhavik Jikadara - Fake News Detection\")\n",
    "    fake = pd.read_csv(os.path.join(folder, \"fake.csv\"))\n",
    "    true = pd.read_csv(os.path.join(folder, \"true.csv\"))\n",
    "    # Assign labels\n",
    "    fake['label'] = 0   # fake → 0\n",
    "    true['label'] = 1   # real → 1\n",
    "    datasets['Bhavik_Jikadara'] = pd.concat([fake, true], ignore_index=True)\n",
    "\n",
    "    # 3. clmentbisaillon_Fakenews\n",
    "    folder = os.path.join(basepath, \"clmentbisaillon_Fakenews\")\n",
    "    real = pd.read_csv(os.path.join(folder, \"True.csv\"))\n",
    "    fake = pd.read_csv(os.path.join(folder, \"Fake.csv\"))\n",
    "    # Drop placeholder empty texts\n",
    "    if 'text' in real.columns:\n",
    "        real = real[real['text'] != \"[empty]\"]\n",
    "    if 'text' in fake.columns:\n",
    "        fake = fake[fake['text'] != \"[empty]\"]\n",
    "    # Assign labels\n",
    "    real['label'] = 1   # real → 1\n",
    "    fake['label'] = 0   # fake → 0\n",
    "    df3 = pd.concat([real, fake], ignore_index=True)\n",
    "    datasets['clmentbisaillon'] = df3\n",
    "\n",
    "    # 4. ErfanMoosaviMonazzah - fake-news-detection-dataset-English\n",
    "    folder = os.path.join(basepath, \"ErfanMoosaviMonazzah - fake-news-detection-dataset-English\")\n",
    "    parts = [\"test.tsv\", \"train.tsv\", \"validation.tsv\"]\n",
    "    dfs = [pd.read_csv(os.path.join(folder, p), sep='\\t') for p in parts]\n",
    "    datasets['ErfanMoosaviMonazzah'] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 5. GonzaloA - fake_news\n",
    "    folder = os.path.join(basepath, \"GonzaloA - fake_news\")\n",
    "    parts = [\"test.csv\", \"train.csv\", \"evaluation.csv\"]\n",
    "    dfs = [pd.read_csv(os.path.join(folder, p), sep=';') for p in parts]\n",
    "    datasets['GonzaloA'] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 6. Hassan Amin - fake_or_real_news.csv\n",
    "    df6 = pd.read_csv(\n",
    "        os.path.join(basepath, \"Hassan Amin-fake_or_real_news.csv/fake_or_real_news.csv\")\n",
    "    )\n",
    "\n",
    "    # The column in this file is likely named 'label' or 'type'; adjust as needed:\n",
    "    # Map the textual labels to integers\n",
    "    df6['label'] = df6['label'].str.upper().map({\n",
    "        'FAKE': 0,\n",
    "        'REAL': 1\n",
    "    })\n",
    "\n",
    "    datasets['Hassan_Amin'] = df6\n",
    "\n",
    "    # 7. Meg Risdal_fake_only\n",
    "    folder = os.path.join(basepath, \"Meg Risdal_fake_only\")\n",
    "    df7 = pd.read_csv(os.path.join(folder, \"fake.csv\"))\n",
    "    # Drop null titles and non‑English entries\n",
    "    if 'titel' in df7.columns:\n",
    "        df7 = df7.dropna(subset=['titel'])\n",
    "        df7 = df7.rename(columns={'titel': 'title'})\n",
    "    if 'language' in df7.columns:\n",
    "        df7 = df7[df7['language'].str.lower() == 'english']\n",
    "    # Assign label: fake-only → 0\n",
    "    df7['label'] = 0\n",
    "    df7 = df7.dropna(subset=['title', 'text'])\n",
    "    df7 = df7[(df7['title'].str.strip() != '') & (df7['text'].str.strip() != '')]\n",
    "    datasets['Meg_Risdal'] = df7\n",
    "\n",
    "    # 8. Ruchi Bhatia_news_articles.csv\n",
    "    df8 = pd.read_csv(os.path.join(basepath, \"Ruchi Bhatia_news_articles.csv/news_articles.csv\"))\n",
    "    \n",
    "    # Clean entries\n",
    "    df8 = df8[~df8['title'].str.lower().isin(['no title', 'newsticker'])]\n",
    "    df8 = df8[df8['text'].notna()]\n",
    "    \n",
    "    # Filter for English language if column exists\n",
    "    if 'language' in df8.columns:\n",
    "        df8 = df8[df8['language'].str.lower() == 'english']\n",
    "    \n",
    "    # Map labels and drop NaNs that result from unmapped values\n",
    "    df8['label'] = df8['label'].str.upper().map({\n",
    "        'FAKE': 0,\n",
    "        'REAL': 1\n",
    "    })\n",
    "    df8 = df8[df8['label'].notna()]  # <--- Remove rows where label is NaN\n",
    "    \n",
    "    datasets['Ruchi_Bhatia'] = df8\n",
    "    \n",
    "    # 9. Saurabh Shahane - Fake_News_Classification\n",
    "    df9 = pd.read_csv(\n",
    "        os.path.join(basepath, \"Saurabh Shahane - Fake_News_Classification\", \"WELFake_Dataset.csv\")\n",
    "    )\n",
    "    \n",
    "    # Drop rows where text or title is NaN\n",
    "    df9 = df9[df9['text'].notna() & df9['title'].notna()]\n",
    "    df9['label'] = 1 - df9['label']\n",
    "    \n",
    "    datasets['Saurabh_Shahane'] = df9\n",
    "\n",
    "    # 10. andyP/fake_news_en_opensources\n",
    "    #datasets['bigFakeNews'] = pd.read_csv(\n",
    "    #    os.path.join(basepath, \"bigFakeNews\", \"dataFiltered.csv\")\n",
    "    #)\n",
    "\n",
    "    return datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fcda8e-dcfc-496b-827e-fe88ae990e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model trained on C+D+E\n",
    "model_path = \"./models/logreg_B.joblib\"\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5043ff76-716c-4bc3-8ecd-89bcf98099f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets fully\n",
    "base_path = '../../data'\n",
    "datasets = load_datasets(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3c4427-a8a4-4d8a-8b4a-68c4d06a66de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model logreg_B_C_D.joblib on individual datasets:\n",
      "\n",
      "Dataset Aadya_Singh: accuracy = 0.97343977\n",
      "Dataset Bhavik_Jikadara: accuracy = 0.99035592\n",
      "Dataset clmentbisaillon: accuracy = 0.99035592\n",
      "Dataset ErfanMoosaviMonazzah: accuracy = 0.97874263\n",
      "Dataset GonzaloA: accuracy = 0.97343977\n",
      "Dataset Hassan_Amin: accuracy = 0.92549329\n",
      "Dataset Meg_Risdal: accuracy = 0.97078399\n",
      "Dataset Ruchi_Bhatia: accuracy = 0.60470325\n",
      "Dataset Saurabh_Shahane: accuracy = 0.97827698\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on each dataset individually\n",
    "print(\"Evaluating model logreg_B_C_D.joblib on individual datasets:\\n\")\n",
    "for name, df in datasets.items():\n",
    "    X = df['title'] + '\\n' + df['text']\n",
    "    y = df['label']\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    print(f\"Dataset {name}: accuracy = {acc:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e27d20-887d-4c7f-a2cd-cec1c4a80f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
