{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dba5b80",
   "metadata": {},
   "source": [
    "# One-Class Fake News Classification with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39f922a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Text feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# One-class classifier and evaluation metrics\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Pipeline utilities\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5b6155",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Paths to dataset splits\n",
    "train_path = \"../../data/ErfanMoosaviMonazzah - fake-news-detection-dataset-English/train.tsv\"\n",
    "val_path   = \"../../data/ErfanMoosaviMonazzah - fake-news-detection-dataset-English/validation.tsv\"\n",
    "test_path  = \"../../data/ErfanMoosaviMonazzah - fake-news-detection-dataset-English/test.tsv\"\n",
    "\n",
    "# Read datasets\n",
    "df_train = pd.read_csv(train_path, sep='\\t', parse_dates=[\"date\"], dayfirst=False)\n",
    "df_val   = pd.read_csv(val_path,   sep='\\t', parse_dates=[\"date\"], dayfirst=False)\n",
    "df_test  = pd.read_csv(test_path,  sep='\\t', parse_dates=[\"date\"], dayfirst=False)\n",
    "\n",
    "# Merge title and text into a single feature\n",
    "def merge_text(row):\n",
    "    return f\"{row['title']} \\n{row['text']}\"\n",
    "\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df['input_text'] = df.apply(merge_text, axis=1)\n",
    "\n",
    "# Prepare data splits\n",
    "y_train = df_train['input_text'][df_train['label'] == 1]  # only real news for one-class training\n",
    "X_val, y_val = df_val['input_text'], df_val['label']\n",
    "X_test, y_test = df_test['input_text'], df_test['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf218ecf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 3) Define grid\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5_000, 10_000, 20_000],\n",
    "    'tfidf__ngram_range' : [(1,1), (1,2)],\n",
    "    'lof__n_neighbors'   : [5, 10, 20],\n",
    "    'lof__contamination' : [0.01, 0.05, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5206871-ac69-409e-a28e-8abb0c340e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Evaluation function\n",
    "def evaluate_lof_params(params):\n",
    "    # build vectorizer + LOF (novelty mode)\n",
    "    vec = TfidfVectorizer(\n",
    "        max_features=params['tfidf__max_features'],\n",
    "        ngram_range=params['tfidf__ngram_range'],\n",
    "        stop_words='english'\n",
    "    )\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=params['lof__n_neighbors'],\n",
    "        contamination=params['lof__contamination'],\n",
    "        novelty=True\n",
    "    )\n",
    "    \n",
    "    # fit on realâ€only training\n",
    "    Xtr = vec.fit_transform(y_train)            # X_train = real+fake âˆª but LOF novelty uses only X_train\n",
    "    lof.fit(Xtr.toarray())                      # LOF requires dense when novelty=True\n",
    "    \n",
    "    # transform validation\n",
    "    Xv = vec.transform(X_val)\n",
    "    raw = lof.predict(Xv.toarray())             # +1=inlierâ†’Real, -1=outlierâ†’Fake\n",
    "    y_pred = np.where(raw==1, 1, 0)\n",
    "    \n",
    "    return accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b69fc440",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.01} â†’ Val acc: 0.4933\n",
      "[2/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.05} â†’ Val acc: 0.5108\n",
      "[3/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.1} â†’ Val acc: 0.5472\n",
      "[4/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.01} â†’ Val acc: 0.4893\n",
      "[5/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.05} â†’ Val acc: 0.5030\n",
      "[6/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.1} â†’ Val acc: 0.5318\n",
      "[7/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.01} â†’ Val acc: 0.4850\n",
      "[8/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.05} â†’ Val acc: 0.4935\n",
      "[9/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.1} â†’ Val acc: 0.5335\n",
      "[10/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.01} â†’ Val acc: 0.4923\n",
      "[11/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.05} â†’ Val acc: 0.5128\n",
      "[12/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.1} â†’ Val acc: 0.5337\n",
      "[13/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.01} â†’ Val acc: 0.4885\n",
      "[14/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.05} â†’ Val acc: 0.5028\n",
      "[15/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.1} â†’ Val acc: 0.5285\n",
      "[16/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.01} â†’ Val acc: 0.4852\n",
      "[17/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.05} â†’ Val acc: 0.4907\n",
      "[18/54] {'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.1} â†’ Val acc: 0.5247\n",
      "[19/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.01} â†’ Val acc: 0.4908\n",
      "[20/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.05} â†’ Val acc: 0.5192\n",
      "[21/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.1} â†’ Val acc: 0.5568\n",
      "[22/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.01} â†’ Val acc: 0.4883\n",
      "[23/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.05} â†’ Val acc: 0.5062\n",
      "[24/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.1} â†’ Val acc: 0.5422\n",
      "[25/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.01} â†’ Val acc: 0.4843\n",
      "[26/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.05} â†’ Val acc: 0.4942\n",
      "[27/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.1} â†’ Val acc: 0.5397\n",
      "[28/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.01} â†’ Val acc: 0.4925\n",
      "[29/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.05} â†’ Val acc: 0.5133\n",
      "[30/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.1} â†’ Val acc: 0.5433\n",
      "[31/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.01} â†’ Val acc: 0.4882\n",
      "[32/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.05} â†’ Val acc: 0.5007\n",
      "[33/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.1} â†’ Val acc: 0.5328\n",
      "[34/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.01} â†’ Val acc: 0.4837\n",
      "[35/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.05} â†’ Val acc: 0.4917\n",
      "[36/54] {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.1} â†’ Val acc: 0.5305\n",
      "[37/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.01} â†’ Val acc: 0.4902\n",
      "[38/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.05} â†’ Val acc: 0.5195\n",
      "[39/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.1} â†’ Val acc: 0.5528\n",
      "[40/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.01} â†’ Val acc: 0.4873\n",
      "[41/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.05} â†’ Val acc: 0.5018\n",
      "[42/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 10, 'lof__contamination': 0.1} â†’ Val acc: 0.5427\n",
      "[43/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.01} â†’ Val acc: 0.4840\n",
      "[44/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.05} â†’ Val acc: 0.4950\n",
      "[45/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 20, 'lof__contamination': 0.1} â†’ Val acc: 0.5417\n",
      "[46/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.01} â†’ Val acc: 0.4903\n",
      "[47/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.05} â†’ Val acc: 0.5147\n",
      "[48/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 5, 'lof__contamination': 0.1} â†’ Val acc: 0.5432\n",
      "[49/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.01} â†’ Val acc: 0.4867\n",
      "[50/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.05} â†’ Val acc: 0.4998\n",
      "[51/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 10, 'lof__contamination': 0.1} â†’ Val acc: 0.5323\n",
      "[52/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.01} â†’ Val acc: 0.4845\n",
      "[53/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.05} â†’ Val acc: 0.4898\n",
      "[54/54] {'tfidf__max_features': 20000, 'tfidf__ngram_range': (1, 2), 'lof__n_neighbors': 20, 'lof__contamination': 0.1} â†’ Val acc: 0.5292\n",
      "\n",
      "âœ… Best validation accuracy: 0.5568333333333333\n",
      "ðŸ“‹ Best parameters: {'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 1), 'lof__n_neighbors': 5, 'lof__contamination': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# 5) bruteâ€force search\n",
    "best_score  = -1\n",
    "best_params = None\n",
    "total = np.prod([len(v) for v in param_grid.values()])\n",
    "i = 0\n",
    "\n",
    "for mf in param_grid['tfidf__max_features']:\n",
    "    for ngr in param_grid['tfidf__ngram_range']:\n",
    "        for nn in param_grid['lof__n_neighbors']:\n",
    "            for cont in param_grid['lof__contamination']:\n",
    "                i += 1\n",
    "                p = {\n",
    "                    'tfidf__max_features': mf,\n",
    "                    'tfidf__ngram_range': ngr,\n",
    "                    'lof__n_neighbors': nn,\n",
    "                    'lof__contamination': cont\n",
    "                }\n",
    "                score = evaluate_lof_params(p)\n",
    "                print(f\"[{i}/{total}] {p} â†’ Val acc: {score:.4f}\")\n",
    "                if score > best_score:\n",
    "                    best_score, best_params = score, p.copy()\n",
    "\n",
    "print(\"\\nâœ… Best validation accuracy:\", best_score)\n",
    "print(\"ðŸ“‹ Best parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80f6f54a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.23 GiB for an array with shape (8267, 20000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Transform and predict on test data\u001b[39;00m\n\u001b[0;32m     29\u001b[0m X_test_vec \u001b[38;5;241m=\u001b[39m final_vec\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m---> 30\u001b[0m raw_test_pred \u001b[38;5;241m=\u001b[39m final_clf\u001b[38;5;241m.\u001b[39mpredict(X_test_vec\u001b[38;5;241m.\u001b[39mtoarray())  \u001b[38;5;66;03m# +1=inlier, -1=outlier\u001b[39;00m\n\u001b[0;32m     31\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(raw_test_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 1=Real, 0=Fake\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1106\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1106\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1327\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.23 GiB for an array with shape (8267, 20000) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# 5. Final model training with best params on train+val and evaluation on test\n",
    "# Combine train (real) and val (all) to retrain\n",
    "combined_texts = pd.concat([y_train, X_val])\n",
    "\n",
    "# Vectorizer with best parameters\n",
    "final_vec = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=best_params['tfidf__ngram_range'],\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# LocalOutlierFactor with best parameters (in novelty mode)\n",
    "final_clf = LocalOutlierFactor(\n",
    "    n_neighbors=best_params['lof__n_neighbors'],\n",
    "#    contamination=best_params['lof__contamination'],\n",
    "    contamination=0.5,\n",
    "    novelty=True  # required for .predict() on new data\n",
    ")\n",
    "\n",
    "# Fit vectorizer on combined data\n",
    "X_combined_vec = final_vec.fit_transform(combined_texts)\n",
    "\n",
    "# LOF requires dense input when novelty=True\n",
    "final_clf.fit(X_combined_vec.toarray())\n",
    "\n",
    "# Transform and predict on test data\n",
    "X_test_vec = final_vec.transform(X_test)\n",
    "raw_test_pred = final_clf.predict(X_test_vec.toarray())  # +1=inlier, -1=outlier\n",
    "test_preds = np.where(raw_test_pred == 1, 1, 0)  # 1=Real, 0=Fake\n",
    "\n",
    "# Evaluation\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(classification_report(y_test, test_preds, target_names=['Fake','Real']))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cc0ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
